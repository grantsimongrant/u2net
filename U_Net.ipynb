{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VidushiBhatia/U-Net-Implementation/blob/main/U_Net_for_Image_Segmentation_From_Scratch_Using_TensorFlow_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1u4Ib9FEK3d"
      },
      "source": [
        "# U-Net 图像分割\n",
        "\n",
        "本notebook实现了U-Net，使用以下资源：\n",
        "* **算法**：Ronnerberger等人，[U-Net](https://arxiv.org/abs/1505.04597)用于生物医学图像分割的卷积神经网络\n",
        "* **数据集**：[oxford iiit宠物数据集](https://www.kaggle.com/tanlikesmath/the-oxfordiiit-pet-dataset)（由Kaggle发布）\n",
        "* **使用的库**：TensorFlow，NumPy，scikit-learn，python-Pillow，imageio，matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmKbyf-METZh",
        "outputId": "20209759-e0f3-4afd-e960-48c1028ecac9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "已加载到 /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SAkwUHEEK3s"
      },
      "source": [
        "# 代码分析\n",
        "\n",
        "- [1 - 包](#1)\n",
        "- [2 - 辅助函数用于数据处理](#2)\n",
        "    - [2.1 - 加载数据](#2.1)\n",
        "    - [2.2 - 预处理数据](#2.2)\n",
        "    - [2.3 - 数据可视化](#2.3)\n",
        "- [3 - 构建U-Net模型](#3)\n",
        "    - [3.1 - U-Net架构](#3.1)\n",
        "    - [3.2 - 损失函数](#3.2)\n",
        "    - [3.3 - 优化器](#3.3)\n",
        "- [4 - 训练模型](#4)\n",
        "    - [4.1 - 数据生成器](#4.1)\n",
        "    - [4.2 - 训练函数](#4.2)\n",
        "    - [4.3 - 模型评估](#4.3)\n",
        "- [5 - 测试模型](#5)"
      ]
    },
    ...
  ]
}- [3 - U-Net架构的辅助函数](#3)
    - [3.1 - U-Net编码器块](#3.1)
    - [3.2 - U-Net解码器块](#3.2)
    - [3.3 - 编译U-Net块](#3.3)
- [4 - 执行上述函数来训练模型](#4)
    - [4.1 - 加载和查看数据](#4.1)
    - [4.2 - 处理数据](#4.2)
    - [4.3 - 拆分训练和测试集](#4.3)
    - [4.4 - 构建U-Net架构](#4.4)
    - [4.5 - 编译和运行模型](#4.5)
- [5 - 评估结果](#5)
    - [5.1 - 偏差-方差检查](#5.1)
    - [5.2 - 查看预测分割结果](#5.2)从sklearn.model_selection中导入train_test_split

## 2 - 数据处理的辅助函数

### 2.1 - 加载数据
* 屏蔽的图片保存为png格式，未屏蔽（原始）图片保存为jpg格式
* 这两者的名称相同，因此我们可以通过对这两个列表进行排序来获取正确的样本

```python
def LoadData (path1, path2):
    """
    在共享文件夹中查找相关的文件名
    返回原始文件和屏蔽文件的两个列表

    """
    # 读取图片文件夹作为列表
    image_dataset = os.listdir(path1)
    mask_dataset = os.listdir(path2)

    # 创建图像和屏蔽文件名的列表
    orig_img = []
    mask_img = []
    for file in image_dataset:
        orig_img.append(file)
    for file in mask_dataset:
        mask_img.append(file)

    # 对列表进行排序，以确保它们的顺序相同（数据集的图片和相应的屏蔽文件名称完全相同）
    orig_img.sort()
    mask_img.sort()
    
    return orig_img, mask_img
```

## 2.2 - 数据预处理"cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIAhtxOjEK3z"
      },
      "outputs": [],
      "source": [
        "def PreprocessData(img, mask, target_shape_img, target_shape_mask, path1, path2):\n",
        "    \"\"\"\n",
        "    处理共享列表和路径中的图像和掩膜\n",
        "    返回一个NumPy数据集，其中图像作为所需大小的3-D数组\n",
        "    请注意，此数据集中的掩膜只有一个通道\n",
        "    \"\"\"\n",
        "    # 获取图像和掩膜的相关维度\n",
        "    m = len(img)                   # 图像数量\n",
        "    i_h,i_w,i_c = target_shape_img  # 获取图像的高度、宽度和通道\n",
        "    m_h,m_w,m_c = target_shape_mask # 获取掩膜的高度、宽度和通道\n",
        "    \n",
        "    # 定义X和Y的图像数量及一个图像的形状\n",
        "    X = np.zeros((m,i_h,i_w,i_c), dtype=np.float32)\n",
        "    y = np.zeros((m,m_h,m_w,m_c), dtype=np.int32)\n",
        "    \n",
        "    # 调整图像和掩膜的大小\n",
        "    for file in img:\n",
        "        # 将图像转换为所需形状的数组（3个通道）\n",
        "        index = img.index(file)\n",
        "        path = os.path.join(path1, file)\n",
        "        single_img = Image.open(path).convert('RGB')\n",
        "        single_img = single_img.resize((i_h,i_w))\n",
        "        single_img = np.reshape(single_img,(i_h,i_w,i_c)) \n",
        "        single_img = single_img/256.\n",
        "        X[index] = single_img\n",
        "        \n",
        "        # 将掩膜转换为所需形状的数组（1个通道）\n",
        "        single_mask_ind = mask[index]\n",
        "        path = os.path.join(path2, single_mask_ind)\n",
        "        single_mask = Image.open(path)\n",
        "        single_mask = single_mask.resize((m_h, m_w))\n",".single_mask = np.reshape(single_mask,(m_h,m_w,m_c))
single_mask = single_mask - 1 # 确保类别从0开始
y[index] = single_mask
return X, y

#3 - 构建U-Net结构

## 3.1 - U-Net编码块

def EncoderMiniBlock(inputs, n_filters=32, dropout_prob=0.3, max_pooling=True):
    """
    该块使用多个卷积层、最大池化和ReLU激活函数来创建学习架构。
    可以添加dropout以进行正则化，以防止过拟合。
    该块返回下一层的激活值和用于解码器的跳跃连接。
    """
    # 使用2个带有ReLU激活函数和HeNormal初始化的卷积层
    # 适当的初始化可以防止梯度爆炸和消失问题
    # 'Same' padding将卷积层的输入填充，使得输出具有相同的高度和宽度（因此大小不会减小）
    conv = Conv2D(n_filters,
                  3,
                  activation='relu',
                  padding='same',
                  kernel_initializer='HeNormal')(inputs)
    conv = Conv2D(n_filters,
                  3,
                  activation='relu',
                  padding='same',
                  kernel_initializer='HeNormal')(conv)

    # 添加最大池化层
    if max_pooling:
        pool = MaxPooling2D(pool_size=(2, 2))(conv)
        return pool, conv
    else:
        return conv, conv```python
def DecoderMiniBlock(prev_layer_input, skip_layer_input, n_filters=32):
    \"""
    返回包含解码器块内容的字符串
    \"\"\"
    
    # Decoder layer combines the information from the corresponding encoder layer and the skip connection from the earlier stage
    up_conv = tf.keras.layers.Conv2DTranspose(filters=n_filters,
                                               kernel_size=(3, 3),
                                               strides=(2, 2),
                                               padding='same')(prev_layer_input)
    
    # Concatenate the skip connection with the decoder layer's output
    merge = tf.keras.layers.Concatenate()([up_conv, skip_layer_input])
    
    conv = tf.keras.layers.Conv2D(filters=n_filters,
                                  kernel_size=(3, 3),
                                  activation='relu',
                                  padding='same',
                                  kernel_initializer='HeNormal')(merge)
    
    conv = tf.keras.layers.BatchNormalization()(conv, training=False)
    
    if dropout_prob > 0:
        conv = tf.keras.layers.Dropout(dropout_prob)(conv)
    
    if max_pooling:
        next_layer = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv)
    else:
        next_layer = conv
    
    skip_connection = conv
    
    return next_layer, skip_connection
```"""
解码器块首先使用转置卷积将图像尺寸放大，然后将结果与编码器块中的跳跃连接结果合并。
通过添加2个具有“same”填充的卷积层，进一步增加网络的深度以实现更好的预测。
该函数返回解码层的输出。

# 从转置卷积层开始，将图像的尺寸增大
up = Conv2DTranspose(
                 n_filters,
                 (3,3),    # 卷积核尺寸
                 strides=(2,2),
                 padding='same')(prev_layer_input)

# 将来自前一个块的跳跃连接合并，以防止信息丢失
merge = concatenate([up, skip_layer_input], axis=3)

# 添加2个具有relu激活函数和HeNormal初始化的卷积层进行进一步处理
# 函数的参数与编码器类似
conv = Conv2D(n_filters, 
                 3,     # 卷积核尺寸
                 activation='relu',
                 padding='same',
                 kernel_initializer='HeNormal')(merge)
conv = Conv2D(n_filters,
                 3,   # 卷积核尺寸
                 activation='relu',
                 padding='same',
                 kernel_initializer='HeNormal')(conv)
return conv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA1JWJ11EK34"
      },
      "source": [
        "<a name='3.3'></a>\n",
        "## 3.3 - 编译U-Net块"
      ]
    }
